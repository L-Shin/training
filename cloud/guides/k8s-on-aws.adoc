= Neo4j on Kubernetes on AWS

== Install Kubernetes on AWS

Install `kops` and `kubectl` before you do anything else.
`kops` is a tool for orchestrating Kubernetes clusters and `kubectl` is a Kubernetes client.

```
brew install kops
brew install kubernetes-cli
```

Once you've done that follow the instructions from the https://github.com/kubernetes/kops/blob/master/docs/aws.md#setup-your-environment[_Setup your environment_^] section of the kops documentation.

```
aws iam create-group --group-name kops
aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess --group-name kops
aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess --group-name kops
aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonRoute53FullAccess --group-name kops
aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess --group-name kops
aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/IAMFullAccess --group-name kops
aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonVPCFullAccess --group-name kops
aws iam create-user --user-name kops
aws iam add-user-to-group --user-name kops --group-name kops
aws iam create-access-key --user-name kops
```

The last command will output some JSON containing a `SecretAccessKey` and `AccessKeyID`.
We'll need to use those values in the next commands

```
aws configure
export AWS_ACCESS_KEY_ID=<AccessKeyID>
export AWS_SECRET_ACCESS_KEY=<SecretAccessKey>
```

Now create an S3 bucket to store the cluster state:

```
export BUCKET_NAME="kubernetes.neo4j.com"
aws s3api create-bucket     --bucket ${BUCKET_NAME}     --region us-east-1
aws s3api put-bucket-versioning --bucket ${BUCKET_NAME}  --versioning-configuration Status=Enabled
```

We're almost there!
Setup the following two environment variables:

```
export NAME=neo4j.k8s.local
export KOPS_STATE_STORE=s3://${BUCKET_NAME}
```

And then run these commands to deploy the cluster:

```
kops create cluster     --zones us-west-2a     ${NAME}
kops update cluster ${NAME} --yes
```

Next we need to run the following command to check the Kubernetes cluster is online:

```
kubectl get nodes
```

It'll take a few minutes so be patient!

Once you get a result from that command it's time to move onto the next step.

== Deploying Neo4j on Kubernetes

Now we're ready to install Neo4j on our Kubernetes cluster.

Once that returns we can run the following command to deploy a Neo4j causal cluster with 3 core servers and no read replicas:

```
kubectl apply -f https://raw.githubusercontent.com/neo4j-contrib/training/master/cloud/k8s/cores/dns.yaml
kubectl apply -f https://raw.githubusercontent.com/neo4j-contrib/training/master/cloud/k8s/cores/statefulset.yaml
```

We can check if that worked by executing the following command:

```
kubectl get pods
```

If we see 3 servers named `neo4j-core-*` then we're good.
We can now tail the logs of our Neo4j servers waiting for them to come online:

```
kubectl logs -l "app=neo4j"
```

We're waiting to see this message:

```
Remote interface available at http://neo4j-core-0.neo4j.default.svc.cluster.local:7474/
```

Once we see that message it's time to have a look at the topology of our cluster:

```
kubectl exec neo4j-core-0 -- bin/cypher-shell --format verbose  \
  "CALL dbms.cluster.overview() \
   YIELD id, role, addresses, groups \
   RETURN id, addresses[0], role, groups"
```

We should see something like this:

```
+---------------------------------------------------------------------------------------------------------------------------+
| id                                     | addresses[0]                                               | role       | groups |
+---------------------------------------------------------------------------------------------------------------------------+
| "2da20d2a-1f07-4fff-8a2b-ddc3c1b87088" | "bolt://neo4j-core-0.neo4j.default.svc.cluster.local:7687" | "LEADER"   | []     |
| "700f3435-12f4-4f3b-8cd0-3d3b2b8e6ba4" | "bolt://neo4j-core-1.neo4j.default.svc.cluster.local:7687" | "FOLLOWER" | []     |
| "896e0437-a2b0-4df5-9c71-c18d0c92e72d" | "bolt://neo4j-core-2.neo4j.default.svc.cluster.local:7687" | "FOLLOWER" | []     |
+---------------------------------------------------------------------------------------------------------------------------+

3 rows available after 306 ms, consumed after another 24 ms
```

Now let's add some read replicas into the mix.
Run the following command:

```
kubectl apply -f https://raw.githubusercontent.com/neo4j-contrib/training/master/cloud/k8s/read-replicas/deployment.yaml
```

Run our topology query again to check the replicas were added:

```
kubectl exec neo-helm-neo4j-core-0 -- bin/cypher-shell --format verbose  \
  "CALL dbms.cluster.overview() \
   YIELD id, role, addresses, groups \
   RETURN id, addresses[0], role, groups"
```

Now we can try scaling the number of replicas

```
kubectl apply deployment neo4j-replica --replicas=3
```

== Backup

We can run a job to backup from one of the servers:

```
kubectl apply -f https://raw.githubusercontent.com/neo4j-contrib/training/master/cloud/k8s/backup.yaml
```

== Monitoring

```
kubectl apply -f https://raw.githubusercontent.com/neo4j-contrib/training/master/cloud/k8s/graphite.yaml
```

Run the following command to port-forward Graphite and Grafana so we can access them from our web browser:

```
kubectl port-forward $(kubectl get pods -l app=neo4j-graphite -o=custom-columns=NAME:.metadata.name | tail -n1) 8080:80 3000:3000
```

Open your browser to http://localhost:3000

You should see the Grafana dashboard.
The credentials are `admin/admin`

Setup the data source to point to `http://localhost:80` and name it `soak`

== Delete the cluster

Don't forget to delete your cluster!

```
kops delete cluster --name ${NAME} --yes
```
