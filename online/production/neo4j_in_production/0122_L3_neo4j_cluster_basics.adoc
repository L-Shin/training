=== (L3) -- (Neo4j Cluster Basics)

++++
<iframe src="https://player.vimeo.com/video/122285723" width="750" height="421" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
++++

Neo4j clustering is comprised of a single Master instance and zero or more Slave instances.
All instances in the cluster have full copies of your data in their local database files.
Each database instance contains the logic needed to coordinate with the other members of the cluster for data replication and election management.

Please note that you need a trusted network to run your Neo4j cluster as currently the network communication is not encrypted.

When performing a write transaction on a slave, each write operation will be synchronized with the master (locks will be acquired on both master and slave).
When the transaction commits it will first be committed on the master and then if successful, on the slave.
To ensure consistency, a slave has to be up to date with the master before performing a write operation.
This is built into the communication protocol between the slave and master so that updates will be applied automatically to a slave communicating with its master.

Write transactions performed directly through the master will execute in the same way as running in normal non-HA mode.
Upon success, the transaction will be pushed out to a configurable number of slaves (default one slave).
This is done optimistically, meaning if the push fails, the transaction will still be successful.
It's also possible to configure the push factor to 0 for higher write performance when writing directly through the master, although this increases the risk of losing any transactions not yet pulled by another slave if the master goes down.

Slaves can also be configured to pull updates asynchronously by setting the `ha.pull_interval` option.

Whenever a Neo4j database becomes unavailable, the other database instances in the cluster will detect that and mark it as temporarily failed.
A database instance that becomes available after being unavailable will automatically catch up with the cluster.
If the master goes down, another member will be elected and switched from slave to master after a quorum has been reached within the cluster.
When the new master has performed its role switch, it will broadcast its availability to all other members of the cluster.
Normally a new master is elected and started within just a few seconds.
During this time, no writes can take place (the writes will block or in rare cases, throw an exception).
The only time this is not true is when an old master had changes that weren't replicated to any other member before becoming unavailable.
If the new master is elected and performs changes before the old master recovers, there will be two "branches" of the database after the point where the old master became unavailable.
The old master will move away its database (its "branch") and download a full copy from the new master to become available as a slave in the cluster.
// help! the section above is confusing

In summary:

* Write transactions can be performed on any database instance in a cluster.
* Neo4j HA is fault tolerant and can continue to operate from any number of machines down to a single machine.
* Slaves will be automatically synchronized with the master on write operations.
* If the master fails, a new master will be elected automatically.
* The cluster automatically handles instances becoming unavailable (for example due to network issues) and also makes sure to accept them as members in the cluster when they are available again.
* Transactions are atomic, consistent and durable but eventually propagated out to other slaves.
* Updates to slaves are eventually consistent by nature but can be configured to be pushed optimistically from the master during commit.
* If the master goes down, any running write transactions will be rolled back and new transactions will block or fail until a new master has become available.
* Reads are highly available and able to handle read load scales with more database instances in the cluster.
